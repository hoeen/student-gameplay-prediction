{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "import pandas as pd\n",
    "pd.reset_option('all')\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import GroupKFold, train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# First model (Catboost)\n",
    "CATS = ['event_name', 'name', 'fqid', 'room_fqid', 'text_fqid']\n",
    "NUMS = ['page', 'room_coor_x', 'room_coor_y', 'screen_coor_x', 'screen_coor_y',\n",
    "        'hover_duration', 'elapsed_time_diff']\n",
    "DIALOGS = ['that', 'this', 'it', 'you','find','found','Found','notebook','Wells','wells','help','need', 'Oh','Ooh','Jo', 'flag', 'can','and','is','the','to']\n",
    "\n",
    "COLUMNS = [\n",
    "    pl.col(\"page\").cast(pl.Float32),\n",
    "    (\n",
    "        (pl.col(\"elapsed_time\") - pl.col(\"elapsed_time\").shift(1))\n",
    "        .fill_null(0)\n",
    "        .clip(0, 1e9)\n",
    "        .over([\"session_id\", \"level\"])\n",
    "        .alias(\"elapsed_time_diff\")\n",
    "    ),\n",
    "    (\n",
    "        (pl.col(\"screen_coor_x\") - pl.col(\"screen_coor_x\").shift(1))\n",
    "        .abs()\n",
    "        .over([\"session_id\", \"level\"])\n",
    "    ),\n",
    "    (\n",
    "        (pl.col(\"screen_coor_y\") - pl.col(\"screen_coor_y\").shift(1))\n",
    "        .abs()\n",
    "        .over([\"session_id\", \"level\"])\n",
    "    ),\n",
    "    pl.col(\"fqid\").fill_null(\"fqid_None\"),\n",
    "    pl.col(\"text_fqid\").fill_null(\"text_fqid_None\")\n",
    "]\n",
    "\n",
    "name_feature = ['basic', 'undefined', 'close', 'open', 'prev', 'next']\n",
    "event_name_feature = ['cutscene_click', 'person_click', 'navigate_click',\n",
    "       'observation_click', 'notification_click', 'object_click',\n",
    "       'object_hover', 'map_hover', 'map_click', 'checkpoint',\n",
    "       'notebook_click']\n",
    "\n",
    "\n",
    "sub_fqid_lists = {'0-4': ['gramps',\n",
    " 'wells',\n",
    " 'toentry',\n",
    " 'groupconvo',\n",
    " 'tomap',\n",
    " 'tostacks',\n",
    " 'tobasement',\n",
    " 'boss',\n",
    " 'cs',\n",
    " 'teddy',\n",
    " 'tunic.historicalsociety',\n",
    " 'plaque',\n",
    " 'directory',\n",
    " 'tunic',\n",
    " 'tunic.kohlcenter',\n",
    " 'plaque.face.date',\n",
    " 'notebook',\n",
    " 'tunic.hub.slip',\n",
    " 'tocollection',\n",
    " 'tunic.capitol_0',\n",
    " 'photo',\n",
    " 'intro',\n",
    " 'retirement_letter',\n",
    " 'togrampa',\n",
    " 'janitor',\n",
    " 'chap1_finale',\n",
    " 'report',\n",
    " 'outtolunch',\n",
    " 'chap1_finale_c',\n",
    " 'block_0',\n",
    " 'doorblock',\n",
    " 'tocloset',\n",
    " 'block_tomap2',\n",
    " 'block_tocollection',\n",
    " 'block_tomap1'],\n",
    "                  '5-12': ['worker',\n",
    " 'archivist',\n",
    " 'gramps',\n",
    " 'toentry',\n",
    " 'tomap',\n",
    " 'tostacks',\n",
    " 'tobasement',\n",
    " 'boss',\n",
    " 'journals',\n",
    " 'businesscards',\n",
    " 'tunic.historicalsociety',\n",
    " 'tofrontdesk',\n",
    " 'plaque',\n",
    " 'tunic.drycleaner',\n",
    " 'tunic.library',\n",
    " 'trigger_scarf',\n",
    " 'reader',\n",
    " 'directory',\n",
    " 'tunic.capitol_1',\n",
    " 'journals.pic_0.next',\n",
    " 'tunic',\n",
    " 'what_happened',\n",
    " 'tunic.kohlcenter',\n",
    " 'tunic.humanecology',\n",
    " 'logbook',\n",
    " 'businesscards.card_0.next',\n",
    " 'journals.hub.topics',\n",
    " 'logbook.page.bingo',\n",
    " 'journals.pic_1.next',\n",
    " 'reader.paper0.next',\n",
    " 'trigger_coffee',\n",
    " 'wellsbadge',\n",
    " 'journals.pic_2.next',\n",
    " 'tomicrofiche',\n",
    " 'tocloset_dirty',\n",
    " 'businesscards.card_bingo.bingo',\n",
    " 'businesscards.card_1.next',\n",
    " 'tunic.hub.slip',\n",
    " 'journals.pic_2.bingo',\n",
    " 'tocollection',\n",
    " 'chap2_finale_c',\n",
    " 'tunic.capitol_0',\n",
    " 'photo',\n",
    " 'reader.paper1.next',\n",
    " 'businesscards.card_bingo.next',\n",
    " 'reader.paper2.bingo',\n",
    " 'magnify',\n",
    " 'janitor',\n",
    " 'tohallway',\n",
    " 'outtolunch',\n",
    " 'reader.paper2.next',\n",
    " 'door_block_talk',\n",
    " 'block_magnify',\n",
    " 'reader.paper0.prev',\n",
    " 'block',\n",
    " 'block_0',\n",
    " 'door_block_clean',\n",
    " 'reader.paper2.prev',\n",
    " 'reader.paper1.prev',\n",
    " 'block_badge',\n",
    " 'block_badge_2',\n",
    " 'block_1'],\n",
    "                  '13-22': ['worker',\n",
    " 'gramps',\n",
    " 'wells',\n",
    " 'toentry',\n",
    " 'confrontation',\n",
    " 'crane_ranger',\n",
    " 'flag_girl',\n",
    " 'tomap',\n",
    " 'tostacks',\n",
    " 'tobasement',\n",
    " 'archivist_glasses',\n",
    " 'boss',\n",
    " 'journals',\n",
    " 'seescratches',\n",
    " 'groupconvo_flag',\n",
    " 'teddy',\n",
    " 'expert',\n",
    " 'businesscards',\n",
    " 'ch3start',\n",
    " 'tunic.historicalsociety',\n",
    " 'tofrontdesk',\n",
    " 'savedteddy',\n",
    " 'plaque',\n",
    " 'glasses',\n",
    " 'tunic.drycleaner',\n",
    " 'reader_flag',\n",
    " 'tunic.library',\n",
    " 'tracks',\n",
    " 'tunic.capitol_2',\n",
    " 'reader',\n",
    " 'directory',\n",
    " 'tunic.capitol_1',\n",
    " 'journals.pic_0.next',\n",
    " 'unlockdoor',\n",
    " 'tunic',\n",
    " 'tunic.kohlcenter',\n",
    " 'tunic.humanecology',\n",
    " 'colorbook',\n",
    " 'logbook',\n",
    " 'businesscards.card_0.next',\n",
    " 'journals.hub.topics',\n",
    " 'journals.pic_1.next',\n",
    " 'journals_flag',\n",
    " 'reader.paper0.next',\n",
    " 'tracks.hub.deer',\n",
    " 'reader_flag.paper0.next',\n",
    " 'journals.pic_2.next',\n",
    " 'tomicrofiche',\n",
    " 'journals_flag.pic_0.bingo',\n",
    " 'tocloset_dirty',\n",
    " 'businesscards.card_1.next',\n",
    " 'tunic.wildlife',\n",
    " 'tunic.hub.slip',\n",
    " 'tocage',\n",
    " 'journals.pic_2.bingo',\n",
    " 'tocollectionflag',\n",
    " 'tocollection',\n",
    " 'chap4_finale_c',\n",
    " 'lockeddoor',\n",
    " 'journals_flag.hub.topics',\n",
    " 'reader_flag.paper2.bingo',\n",
    " 'photo',\n",
    " 'tunic.flaghouse',\n",
    " 'reader.paper1.next',\n",
    " 'directory.closeup.archivist',\n",
    " 'businesscards.card_bingo.next',\n",
    " 'remove_cup',\n",
    " 'journals_flag.pic_0.next',\n",
    " 'coffee',\n",
    " 'key',\n",
    " 'reader_flag.paper1.next',\n",
    " 'tohallway',\n",
    " 'outtolunch',\n",
    " 'journals_flag.hub.topics_old',\n",
    " 'journals_flag.pic_1.next',\n",
    " 'reader.paper2.next',\n",
    " 'reader_flag.paper2.next',\n",
    " 'journals_flag.pic_1.bingo',\n",
    " 'journals_flag.pic_2.next',\n",
    " 'journals_flag.pic_2.bingo',\n",
    " 'reader.paper0.prev',\n",
    " 'reader_flag.paper0.prev',\n",
    " 'reader.paper2.prev',\n",
    " 'reader.paper1.prev',\n",
    " 'reader_flag.paper2.prev',\n",
    " 'reader_flag.paper1.prev',\n",
    " 'journals_flag.pic_0_old.next',\n",
    " 'journals_flag.pic_1_old.next',\n",
    " 'block_nelson',\n",
    " 'journals_flag.pic_2_old.next',\n",
    " 'need_glasses',\n",
    " 'fox'],\n",
    "                 }\n",
    "\n",
    "sub_room_lists = {'0-4': ['tunic.historicalsociety.entry',\n",
    " 'tunic.historicalsociety.stacks',\n",
    " 'tunic.historicalsociety.basement',\n",
    " 'tunic.kohlcenter.halloffame',\n",
    " 'tunic.historicalsociety.collection',\n",
    " 'tunic.historicalsociety.closet',\n",
    " 'tunic.capitol_0.hall'],\n",
    "                  '5-12': ['tunic.historicalsociety.entry',\n",
    " 'tunic.library.frontdesk',\n",
    " 'tunic.historicalsociety.frontdesk',\n",
    " 'tunic.historicalsociety.stacks',\n",
    " 'tunic.historicalsociety.closet_dirty',\n",
    " 'tunic.humanecology.frontdesk',\n",
    " 'tunic.historicalsociety.basement',\n",
    " 'tunic.kohlcenter.halloffame',\n",
    " 'tunic.library.microfiche',\n",
    " 'tunic.drycleaner.frontdesk',\n",
    " 'tunic.historicalsociety.collection',\n",
    " 'tunic.capitol_1.hall',\n",
    " 'tunic.capitol_0.hall'],\n",
    "                  '13-22': ['tunic.historicalsociety.entry',\n",
    " 'tunic.wildlife.center',\n",
    " 'tunic.historicalsociety.cage',\n",
    " 'tunic.library.frontdesk',\n",
    " 'tunic.historicalsociety.frontdesk',\n",
    " 'tunic.historicalsociety.stacks',\n",
    " 'tunic.historicalsociety.closet_dirty',\n",
    " 'tunic.humanecology.frontdesk',\n",
    " 'tunic.historicalsociety.basement',\n",
    " 'tunic.kohlcenter.halloffame',\n",
    " 'tunic.library.microfiche',\n",
    " 'tunic.drycleaner.frontdesk',\n",
    " 'tunic.historicalsociety.collection',\n",
    " 'tunic.flaghouse.entry',\n",
    " 'tunic.historicalsociety.collection_flag',\n",
    " 'tunic.capitol_1.hall',\n",
    " 'tunic.capitol_2.hall'],\n",
    "                 }\n",
    "\n",
    "\n",
    "sub_text_lists = {'0-4': ['tunic.historicalsociety.entry.groupconvo',\n",
    " 'tunic.historicalsociety.collection.cs',\n",
    " 'tunic.historicalsociety.collection.gramps.found',\n",
    " 'tunic.historicalsociety.closet.gramps.intro_0_cs_0',\n",
    " 'tunic.historicalsociety.closet.teddy.intro_0_cs_0',\n",
    " 'tunic.historicalsociety.closet.intro',\n",
    " 'tunic.historicalsociety.closet.retirement_letter.hub',\n",
    " 'tunic.historicalsociety.collection.tunic.slip',\n",
    " 'tunic.kohlcenter.halloffame.plaque.face.date',\n",
    " 'tunic.kohlcenter.halloffame.togrampa',\n",
    " 'tunic.historicalsociety.collection.gramps.lost',\n",
    " 'tunic.historicalsociety.closet.notebook',\n",
    " 'tunic.historicalsociety.basement.janitor',\n",
    " 'tunic.historicalsociety.stacks.outtolunch',\n",
    " 'tunic.historicalsociety.closet.photo',\n",
    " 'tunic.historicalsociety.collection.tunic',\n",
    " 'tunic.historicalsociety.closet.teddy.intro_0_cs_5',\n",
    " 'tunic.historicalsociety.entry.wells.talktogramps',\n",
    " 'tunic.historicalsociety.entry.boss.talktogramps',\n",
    " 'tunic.historicalsociety.closet.doorblock',\n",
    " 'tunic.historicalsociety.entry.block_tomap2',\n",
    " 'tunic.historicalsociety.entry.block_tocollection',\n",
    " 'tunic.historicalsociety.entry.block_tomap1',\n",
    " 'tunic.historicalsociety.collection.gramps.look_0',\n",
    " 'tunic.kohlcenter.halloffame.block_0',\n",
    " 'tunic.capitol_0.hall.chap1_finale_c',\n",
    " 'tunic.historicalsociety.entry.gramps.hub'],\n",
    "               '5-12': ['tunic.historicalsociety.frontdesk.archivist.newspaper',\n",
    " 'tunic.historicalsociety.frontdesk.archivist.have_glass',\n",
    " 'tunic.drycleaner.frontdesk.worker.hub',\n",
    " 'tunic.historicalsociety.closet_dirty.gramps.news',\n",
    " 'tunic.humanecology.frontdesk.worker.intro',\n",
    " 'tunic.library.frontdesk.worker.hello',\n",
    " 'tunic.library.frontdesk.worker.wells',\n",
    " 'tunic.historicalsociety.frontdesk.archivist.hello',\n",
    " 'tunic.historicalsociety.closet_dirty.trigger_scarf',\n",
    " 'tunic.drycleaner.frontdesk.worker.done',\n",
    " 'tunic.historicalsociety.closet_dirty.what_happened',\n",
    " 'tunic.historicalsociety.stacks.journals.pic_2.bingo',\n",
    " 'tunic.humanecology.frontdesk.worker.badger',\n",
    " 'tunic.historicalsociety.closet_dirty.trigger_coffee',\n",
    " 'tunic.drycleaner.frontdesk.logbook.page.bingo',\n",
    " 'tunic.library.microfiche.reader.paper2.bingo',\n",
    " 'tunic.historicalsociety.closet_dirty.gramps.helpclean',\n",
    " 'tunic.historicalsociety.frontdesk.archivist.have_glass_recap',\n",
    " 'tunic.historicalsociety.frontdesk.magnify',\n",
    " 'tunic.humanecology.frontdesk.businesscards.card_bingo.bingo',\n",
    " 'tunic.library.frontdesk.wellsbadge.hub',\n",
    " 'tunic.capitol_1.hall.boss.haveyougotit',\n",
    " 'tunic.historicalsociety.basement.janitor',\n",
    " 'tunic.historicalsociety.closet_dirty.photo',\n",
    " 'tunic.historicalsociety.stacks.outtolunch',\n",
    " 'tunic.library.frontdesk.worker.wells_recap',\n",
    " 'tunic.capitol_0.hall.boss.talktogramps',\n",
    " 'tunic.historicalsociety.closet_dirty.gramps.archivist',\n",
    " 'tunic.historicalsociety.closet_dirty.door_block_talk',\n",
    " 'tunic.historicalsociety.frontdesk.archivist.need_glass_0',\n",
    " 'tunic.historicalsociety.frontdesk.block_magnify',\n",
    " 'tunic.historicalsociety.frontdesk.archivist.foundtheodora',\n",
    " 'tunic.historicalsociety.closet_dirty.gramps.nothing',\n",
    " 'tunic.historicalsociety.closet_dirty.door_block_clean',\n",
    " 'tunic.library.frontdesk.worker.hello_short',\n",
    " 'tunic.historicalsociety.stacks.block',\n",
    " 'tunic.historicalsociety.frontdesk.archivist.need_glass_1',\n",
    " 'tunic.historicalsociety.frontdesk.archivist.newspaper_recap',\n",
    " 'tunic.drycleaner.frontdesk.worker.done2',\n",
    " 'tunic.humanecology.frontdesk.block_0',\n",
    " 'tunic.library.frontdesk.worker.preflag',\n",
    " 'tunic.drycleaner.frontdesk.worker.takealook',\n",
    " 'tunic.library.frontdesk.worker.droppedbadge',\n",
    " 'tunic.library.microfiche.block_0',\n",
    " 'tunic.library.frontdesk.block_badge',\n",
    " 'tunic.library.frontdesk.block_badge_2',\n",
    " 'tunic.capitol_1.hall.chap2_finale_c',\n",
    " 'tunic.drycleaner.frontdesk.block_0',\n",
    " 'tunic.humanecology.frontdesk.block_1',\n",
    " 'tunic.drycleaner.frontdesk.block_1'],\n",
    "               '13-22': ['tunic.historicalsociety.cage.confrontation',\n",
    " 'tunic.wildlife.center.crane_ranger.crane',\n",
    " 'tunic.wildlife.center.wells.nodeer',\n",
    " 'tunic.historicalsociety.frontdesk.archivist_glasses.confrontation',\n",
    " 'tunic.historicalsociety.basement.seescratches',\n",
    " 'tunic.flaghouse.entry.flag_girl.hello',\n",
    " 'tunic.historicalsociety.basement.ch3start',\n",
    " 'tunic.historicalsociety.entry.groupconvo_flag',\n",
    " 'tunic.historicalsociety.collection_flag.gramps.flag',\n",
    " 'tunic.historicalsociety.basement.savedteddy',\n",
    " 'tunic.library.frontdesk.worker.nelson',\n",
    " 'tunic.wildlife.center.expert.removed_cup',\n",
    " 'tunic.library.frontdesk.worker.flag',\n",
    " 'tunic.historicalsociety.entry.boss.flag',\n",
    " 'tunic.flaghouse.entry.flag_girl.symbol',\n",
    " 'tunic.wildlife.center.wells.animals',\n",
    " 'tunic.historicalsociety.cage.glasses.afterteddy',\n",
    " 'tunic.historicalsociety.cage.teddy.trapped',\n",
    " 'tunic.historicalsociety.cage.unlockdoor',\n",
    " 'tunic.historicalsociety.stacks.journals.pic_2.bingo',\n",
    " 'tunic.historicalsociety.entry.wells.flag',\n",
    " 'tunic.humanecology.frontdesk.worker.badger',\n",
    " 'tunic.historicalsociety.stacks.journals_flag.pic_0.bingo',\n",
    " 'tunic.historicalsociety.entry.directory.closeup.archivist',\n",
    " 'tunic.capitol_2.hall.boss.haveyougotit',\n",
    " 'tunic.wildlife.center.wells.nodeer_recap',\n",
    " 'tunic.historicalsociety.cage.glasses.beforeteddy',\n",
    " 'tunic.wildlife.center.expert.recap',\n",
    " 'tunic.historicalsociety.stacks.journals_flag.pic_1.bingo',\n",
    " 'tunic.historicalsociety.cage.lockeddoor',\n",
    " 'tunic.historicalsociety.stacks.journals_flag.pic_2.bingo',\n",
    " 'tunic.wildlife.center.remove_cup',\n",
    " 'tunic.wildlife.center.tracks.hub.deer',\n",
    " 'tunic.historicalsociety.frontdesk.key',\n",
    " 'tunic.library.microfiche.reader_flag.paper2.bingo',\n",
    " 'tunic.flaghouse.entry.colorbook',\n",
    " 'tunic.wildlife.center.coffee',\n",
    " 'tunic.historicalsociety.collection_flag.gramps.recap',\n",
    " 'tunic.wildlife.center.wells.animals2',\n",
    " 'tunic.flaghouse.entry.flag_girl.symbol_recap',\n",
    " 'tunic.historicalsociety.closet_dirty.photo',\n",
    " 'tunic.historicalsociety.stacks.outtolunch',\n",
    " 'tunic.historicalsociety.frontdesk.archivist_glasses.confrontation_recap',\n",
    " 'tunic.historicalsociety.entry.boss.flag_recap',\n",
    " 'tunic.capitol_1.hall.boss.writeitup',\n",
    " 'tunic.library.frontdesk.worker.nelson_recap',\n",
    " 'tunic.historicalsociety.entry.wells.flag_recap',\n",
    " 'tunic.drycleaner.frontdesk.worker.done2',\n",
    " 'tunic.library.frontdesk.worker.flag_recap',\n",
    " 'tunic.library.frontdesk.worker.preflag',\n",
    " 'tunic.historicalsociety.basement.gramps.seeyalater',\n",
    " 'tunic.flaghouse.entry.flag_girl.hello_recap',\n",
    " 'tunic.historicalsociety.basement.gramps.whatdo',\n",
    " 'tunic.library.frontdesk.block_nelson',\n",
    " 'tunic.historicalsociety.cage.need_glasses',\n",
    " 'tunic.capitol_2.hall.chap4_finale_c',\n",
    " 'tunic.wildlife.center.fox.concern']\n",
    "              }\n",
    "\n",
    "\n",
    "SUB_LEVELS = {'0-4': [1, 2, 3, 4],\n",
    "              '5-12': [5, 6, 7, 8, 9, 10, 11, 12],\n",
    "              '13-22': [13, 14, 15, 16, 17, 18, 19, 20, 21, 22]}\n",
    "level_groups = [\"0-4\", \"5-12\", \"13-22\"]\n",
    "\n",
    "\n",
    "def feature_engineer(x, grp, use_extra, feature_suffix):\n",
    "    LEVELS = SUB_LEVELS[grp]\n",
    "    text_lists = sub_text_lists[grp]\n",
    "    room_lists = sub_room_lists[grp]\n",
    "    fqid_lists = sub_fqid_lists[grp]\n",
    "    aggs = [\n",
    "\n",
    "        pl.col(\"index\").count().alias(f\"session_number_{feature_suffix}\"),\n",
    "\n",
    "        *[pl.col('index').filter(pl.col('text').str.contains(c)).count().alias(f'word_{c}') for c in DIALOGS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(c))).mean().alias(f'word_mean_{c}') for c in\n",
    "          DIALOGS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(c))).std().alias(f'word_std_{c}') for c in\n",
    "          DIALOGS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(c))).max().alias(f'word_max_{c}') for c in\n",
    "          DIALOGS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(c))).min().alias(f'word_min_{c}') for c in\n",
    "          DIALOGS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(c))).sum().alias(f'word_sum_{c}') for c in\n",
    "          DIALOGS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(c))).median().alias(f'word_median_{c}') for c\n",
    "          in DIALOGS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(c))).skew().alias(f'word_skew_{c}') for c\n",
    "          in DIALOGS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(c))).kurtosis().alias(f'word_kurtosis_{c}') for c\n",
    "          in DIALOGS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(c))).quantile(0.25).alias(f'word_quant25_{c}') for c\n",
    "          in DIALOGS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(c))).quantile(0.3).alias(f'word_quant3_{c}') for c\n",
    "          in DIALOGS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(c))).quantile(0.8).alias(f'word_quant8_{c}') for c\n",
    "          in DIALOGS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(c))).quantile(0.5).alias(f'word_quant5_{c}') for c\n",
    "          in DIALOGS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(c))).quantile(0.65).alias(f'word_quant65_{c}') for c\n",
    "          in DIALOGS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(c))).quantile(0.75).alias(f'word_quant75_{c}') for c\n",
    "          in DIALOGS],\n",
    "        *[(pl.col(\"elapsed_time_diff\").filter(pl.col(\"text\") == c).max() - pl.col(\"elapsed_time_diff\").filter(\n",
    "            pl.col(\"text\") == c).min()).alias(f\"{c}_word_max_min_{feature_suffix}\") for c in DIALOGS],\n",
    "\n",
    "        *[pl.col(c).drop_nulls().n_unique().alias(f\"{c}_unique_{feature_suffix}\") for c in CATS],\n",
    "\n",
    "        *[pl.col(c).mean().alias(f\"{c}_mean_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).std().alias(f\"{c}_std_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).min().alias(f\"{c}_min_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).max().alias(f\"{c}_max_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).median().alias(f\"{c}_median_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).skew().alias(f\"{c}_skew_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).kurtosis().alias(f\"{c}_kurtosis_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).quantile(0.25).alias(f\"{c}_quant25_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).quantile(0.75).alias(f\"{c}_quant75_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).quantile(0.3).alias(f\"{c}_quant3_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).quantile(0.8).alias(f\"{c}_quant8_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).quantile(0.5).alias(f\"{c}_quant5_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).quantile(0.65).alias(f\"{c}_quant65_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).var().alias(f\"{c}_var_{feature_suffix}\") for c in NUMS],\n",
    "        *[(pl.col(c).max() - pl.col(c).min()).alias(f\"{c}_max_min_{feature_suffix}\") for c in NUMS],\n",
    " \n",
    "\n",
    "        *[pl.col(\"fqid\").filter(pl.col(\"fqid\") == c).count().alias(f\"{c}_fqid_counts{feature_suffix}\")\n",
    "          for c in fqid_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for\n",
    "          c in fqid_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for\n",
    "          c in fqid_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for\n",
    "          c in fqid_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).median().alias(f\"{c}_ET_median_{feature_suffix}\") for\n",
    "          c in fqid_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).min().alias(f\"{c}_ET_min_{feature_suffix}\") for\n",
    "          c in fqid_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for\n",
    "          c in fqid_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).skew().alias(f\"{c}_ET_skew_{feature_suffix}\") for\n",
    "          c in fqid_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).kurtosis().alias(f\"{c}_ET_kurtosis_{feature_suffix}\") for\n",
    "          c in fqid_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).quantile(0.25).alias(f\"{c}_ET_quant25_{feature_suffix}\") for\n",
    "          c in fqid_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).quantile(0.75).alias(f\"{c}_ET_quant75_{feature_suffix}\") for \n",
    "          c in fqid_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).quantile(0.3).alias(f\"{c}_ET_quant3_{feature_suffix}\") for\n",
    "          c in fqid_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).quantile(0.8).alias(f\"{c}_ET_quant8_{feature_suffix}\") for \n",
    "          c in fqid_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).quantile(0.5).alias(f\"{c}_ET_quant5_{feature_suffix}\") for\n",
    "          c in fqid_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).quantile(0.65).alias(f\"{c}_ET_quant65_{feature_suffix}\") for\n",
    "          c in fqid_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).var().alias(f\"{c}_ET_var_{feature_suffix}\") for\n",
    "          c in fqid_lists],\n",
    "        *[(pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).max() - pl.col(\"elapsed_time_diff\").filter(\n",
    "            pl.col(\"fqid\") == c).min()).alias(f\"{c}_ET_max_min_{feature_suffix}\") for c in fqid_lists],\n",
    "\n",
    "\n",
    "\n",
    "        *[pl.col(\"text_fqid\").filter(pl.col(\"text_fqid\") == c).count().alias(f\"{c}_text_fqid_counts{feature_suffix}\")\n",
    "          for\n",
    "          c in text_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for\n",
    "          c in text_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for\n",
    "          c in text_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for\n",
    "          c in text_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).median().alias(f\"{c}_ET_median_{feature_suffix}\")\n",
    "          for\n",
    "          c in text_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).min().alias(f\"{c}_ET_min_{feature_suffix}\") for\n",
    "          c in text_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for\n",
    "          c in text_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).skew().alias(f\"{c}_ET_skew_{feature_suffix}\") for\n",
    "          c in text_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).kurtosis().alias(f\"{c}_ET_kurtosis_{feature_suffix}\") for\n",
    "          c in text_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).quantile(0.25).alias(f\"{c}_ET_quant25_{feature_suffix}\") for\n",
    "          c in text_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).quantile(0.75).alias(f\"{c}_ET_quant75_{feature_suffix}\") for\n",
    "          c in text_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).quantile(0.3).alias(f\"{c}_ET_quant3_{feature_suffix}\") for\n",
    "          c in text_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).quantile(0.8).alias(f\"{c}_ET_quant8_{feature_suffix}\") for\n",
    "          c in text_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).quantile(0.5).alias(f\"{c}_ET_quant5_{feature_suffix}\") for\n",
    "          c in text_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).quantile(0.65).alias(f\"{c}_ET_quant65_{feature_suffix}\") for\n",
    "          c in text_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).var().alias(f\"{c}_ET_var_{feature_suffix}\") for\n",
    "          c in text_lists],\n",
    "        *[(pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).max() - pl.col(\"elapsed_time_diff\").filter(\n",
    "            pl.col(\"text_fqid\") == c).min()).alias(f\"{c}_ET_max_min_{feature_suffix}\") for c in text_lists],\n",
    "\n",
    "\n",
    "        *[pl.col(\"room_fqid\").filter(pl.col(\"room_fqid\") == c).count().alias(f\"{c}_room_fqid_counts{feature_suffix}\")\n",
    "          for c in room_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for\n",
    "          c in room_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for\n",
    "          c in room_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for\n",
    "          c in room_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).median().alias(f\"{c}_ET_median_{feature_suffix}\") for\n",
    "          c in room_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for\n",
    "          c in room_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).min().alias(f\"{c}_ET_min_{feature_suffix}\") for\n",
    "          c in room_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).skew().alias(f\"{c}_ET_skew_{feature_suffix}\") for\n",
    "          c in room_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).kurtosis().alias(f\"{c}_ET_kurtosis_{feature_suffix}\") for\n",
    "          c in room_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).quantile(0.25).alias(f\"{c}_ET_quant25_{feature_suffix}\") for\n",
    "          c in room_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).quantile(0.75).alias(f\"{c}_ET_quant75_{feature_suffix}\") for\n",
    "          c in room_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).quantile(0.3).alias(f\"{c}_ET_quant3_{feature_suffix}\") for\n",
    "          c in room_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).quantile(0.8).alias(f\"{c}_ET_quant8_{feature_suffix}\") for\n",
    "          c in room_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).quantile(0.5).alias(f\"{c}_ET_quant5_{feature_suffix}\") for\n",
    "          c in room_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).quantile(0.65).alias(f\"{c}_ET_quant65_{feature_suffix}\") for\n",
    "          c in room_lists], \n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).var().alias(f\"{c}_ET_var_{feature_suffix}\") for\n",
    "          c in room_lists],\n",
    "        *[(pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).max() - pl.col(\"elapsed_time_diff\").filter(\n",
    "            pl.col(\"room_fqid\") == c).min()).alias(f\"{c}_ET_max_min_{feature_suffix}\") for c in room_lists],\n",
    "\n",
    "        \n",
    "\n",
    "        *[pl.col(\"event_name\").filter(pl.col(\"event_name\") == c).count().alias(f\"{c}_event_name_counts{feature_suffix}\")\n",
    "          for c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for\n",
    "          c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\")\n",
    "          for\n",
    "          c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for\n",
    "          c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).median().alias(\n",
    "            f\"{c}_ET_median_{feature_suffix}\") for\n",
    "          c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for\n",
    "          c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).min().alias(f\"{c}_ET_min_{feature_suffix}\") for\n",
    "          c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).skew().alias(f\"{c}_ET_skew_{feature_suffix}\") for\n",
    "          c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).kurtosis().alias(f\"{c}_ET_kurtosis_{feature_suffix}\") for\n",
    "          c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).quantile(0.25).alias(f\"{c}_ET_quant25_{feature_suffix}\") for\n",
    "          c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).quantile(0.75).alias(f\"{c}_ET_quant75_{feature_suffix}\") for\n",
    "          c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).quantile(0.3).alias(f\"{c}_ET_quant3_{feature_suffix}\") for\n",
    "          c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).quantile(0.8).alias(f\"{c}_ET_quant8_{feature_suffix}\") for\n",
    "          c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).quantile(0.5).alias(f\"{c}_ET_quant5_{feature_suffix}\") for\n",
    "          c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).quantile(0.65).alias(f\"{c}_ET_quant65_{feature_suffix}\") for\n",
    "          c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).var().alias(f\"{c}_ET_var_{feature_suffix}\") for\n",
    "          c in event_name_feature],\n",
    "        *[(pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).max() - pl.col(\"elapsed_time_diff\").filter(\n",
    "            pl.col(\"event_name\") == c).min()).alias(f\"{c}_ET_max_min_{feature_suffix}\") for c in event_name_feature],\n",
    "\n",
    "\n",
    "        *[pl.col(\"name\").filter(pl.col(\"name\") == c).count().alias(f\"{c}_name_counts{feature_suffix}\") for c in\n",
    "          name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for c in\n",
    "          name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for c in\n",
    "          name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for c in\n",
    "          name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).median().alias(f\"{c}_ET_median_{feature_suffix}\") for\n",
    "          c in\n",
    "          name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).min().alias(f\"{c}_ET_min_{feature_suffix}\") for\n",
    "          c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for\n",
    "          c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).skew().alias(f\"{c}_ET_skew_{feature_suffix}\") for\n",
    "          c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).kurtosis().alias(f\"{c}_ET_kurtosis_{feature_suffix}\") for\n",
    "          c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).quantile(0.25).alias(f\"{c}_ET_quant25_{feature_suffix}\") for\n",
    "          c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).quantile(0.75).alias(f\"{c}_ET_quant75_{feature_suffix}\") for\n",
    "          c in name_feature],\n",
    "         *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).quantile(0.3).alias(f\"{c}_ET_quant3_{feature_suffix}\") for\n",
    "          c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).quantile(0.8).alias(f\"{c}_ET_quant8_{feature_suffix}\") for\n",
    "          c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).quantile(0.5).alias(f\"{c}_ET_quant5_{feature_suffix}\") for\n",
    "          c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).quantile(0.65).alias(f\"{c}_ET_quant65_{feature_suffix}\") for\n",
    "          c in name_feature], \n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).var().alias(f\"{c}_ET_var_{feature_suffix}\") for\n",
    "          c in name_feature],\n",
    "        *[(pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).max() - pl.col(\"elapsed_time_diff\").filter(\n",
    "            pl.col(\"name\") == c).min()).alias(f\"{c}_ET_max_min_{feature_suffix}\") for c in name_feature],\n",
    "\n",
    "\n",
    "        *[pl.col(\"level\").filter(pl.col(\"level\") == c).count().alias(f\"{c}_LEVEL_count{feature_suffix}\") for c in\n",
    "          LEVELS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for c in\n",
    "          LEVELS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for c\n",
    "          in\n",
    "          LEVELS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for c in\n",
    "          LEVELS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).median().alias(f\"{c}_ET_median_{feature_suffix}\") for\n",
    "          c in\n",
    "          LEVELS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).min().alias(f\"{c}_ET_min_{feature_suffix}\") for\n",
    "          c in LEVELS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for\n",
    "          c in LEVELS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).skew().alias(f\"{c}_ET_skew_{feature_suffix}\") for\n",
    "          c in LEVELS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).kurtosis().alias(f\"{c}_ET_kurtosis_{feature_suffix}\") for\n",
    "          c in LEVELS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).quantile(0.25).alias(f\"{c}_ET_quant25_{feature_suffix}\") for\n",
    "          c in LEVELS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).quantile(0.75).alias(f\"{c}_ET_quant75_{feature_suffix}\") for\n",
    "          c in LEVELS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).quantile(0.3).alias(f\"{c}_ET_quant3_{feature_suffix}\") for\n",
    "          c in LEVELS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).quantile(0.8).alias(f\"{c}_ET_quant8_{feature_suffix}\") for\n",
    "          c in LEVELS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).quantile(0.5).alias(f\"{c}_ET_quant5_{feature_suffix}\") for\n",
    "          c in LEVELS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).quantile(0.65).alias(f\"{c}_ET_quant65_{feature_suffix}\") for\n",
    "          c in LEVELS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).var().alias(f\"{c}_var_{feature_suffix}\") for\n",
    "          c in LEVELS],\n",
    "        *[(pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).max() - pl.col(\"elapsed_time_diff\").filter(\n",
    "            pl.col(\"level\") == c).min()).alias(f\"{c}_ET_max_min_{feature_suffix}\") for c in LEVELS],\n",
    "\n",
    "\n",
    "        *[pl.col(\"level_group\").filter(pl.col(\"level_group\") == c).count().alias(\n",
    "            f\"{c}_LEVEL_group_count{feature_suffix}\") for c in\n",
    "          level_groups],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level_group\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for\n",
    "          c in\n",
    "          level_groups],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level_group\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\")\n",
    "          for c in\n",
    "          level_groups],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level_group\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for\n",
    "          c in\n",
    "          level_groups],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level_group\") == c).median().alias(\n",
    "            f\"{c}_ET_median_{feature_suffix}\") for c in\n",
    "          level_groups],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level_group\") == c).min().alias(f\"{c}_ET_min_{feature_suffix}\") for\n",
    "          c in level_groups],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level_group\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for\n",
    "          c in level_groups],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level_group\") == c).skew().alias(f\"{c}_ET_skew_{feature_suffix}\") for\n",
    "          c in level_groups],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level_group\") == c).kurtosis().alias(f\"{c}_ET_kurtosis_{feature_suffix}\") for\n",
    "          c in level_groups],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level_group\") == c).quantile(0.25).alias(f\"{c}_ET_quant25_{feature_suffix}\") for\n",
    "          c in level_groups],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level_group\") == c).quantile(0.75).alias(f\"{c}_ET_quant75_{feature_suffix}\") for\n",
    "          c in level_groups],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level_group\") == c).quantile(0.3).alias(f\"{c}_ET_quant3_{feature_suffix}\") for\n",
    "          c in level_groups],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level_group\") == c).quantile(0.8).alias(f\"{c}_ET_quant8_{feature_suffix}\") for\n",
    "          c in level_groups],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level_group\") == c).quantile(0.5).alias(f\"{c}_ET_quant5_{feature_suffix}\") for\n",
    "          c in level_groups],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level_group\") == c).quantile(0.65).alias(f\"{c}_ET_quant65_{feature_suffix}\") for\n",
    "          c in level_groups],\n",
    "        *[(pl.col(\"elapsed_time_diff\").filter(pl.col(\"level_group\") == c).max() - pl.col(\"elapsed_time_diff\").filter(\n",
    "            pl.col(\"level_group\") == c).min()).alias(f\"{c}_ET_max_min_{feature_suffix}\") for c in level_groups],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level_group\") == c).var().alias(f\"{c}_ET_var_{feature_suffix}\") for\n",
    "          c in level_groups]\n",
    "\n",
    "    ]\n",
    "    # df = x.groupby(['session_id']).agg(aggs).sort_values(\"session_id\")\n",
    "\n",
    "    df = x.with_columns(COLUMNS).groupby(['session_id'], maintain_order=True).agg(aggs).sort(\"session_id\")\n",
    "\n",
    "    # Time features - Year, month, ...\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"session_id\").apply(lambda x: int(str(x)[:2])).alias('year'),\n",
    "        pl.col(\"session_id\").apply(lambda x: int(str(x)[2:4])+1).alias('month'),\n",
    "        pl.col(\"session_id\").apply(lambda x: int(str(x)[4:6])).alias('day'),\n",
    "        pl.col(\"session_id\").apply(lambda x: int(str(x)[6:8])).alias('hour'),\n",
    "        pl.col(\"session_id\").apply(lambda x: int(str(x)[8:10])).alias('minute'),\n",
    "        pl.col(\"session_id\").apply(lambda x: int(str(x)[10:12])).alias('second'),\n",
    "        pl.col(\"session_id\").apply(lambda x: int(str(x)[12:])).alias('id_anonymous'),\n",
    "    )\n",
    "\n",
    "    if use_extra:\n",
    "        if grp == '5-12':\n",
    "            aggs = [\n",
    "                pl.col(\"elapsed_time\").filter((pl.col(\"text\") == \"Here's the log book.\")\n",
    "                                              | (pl.col(\"fqid\") == 'logbook.page.bingo'))\n",
    "                    .apply(lambda s: s.max() - s.min()).alias(\"logbook_bingo_duration\"),\n",
    "                pl.col(\"index\").filter(\n",
    "                    (pl.col(\"text\") == \"Here's the log book.\") | (pl.col(\"fqid\") == 'logbook.page.bingo')).apply(\n",
    "                    lambda s: s.max() - s.min()).alias(\"logbook_bingo_indexCount\"),\n",
    "                pl.col(\"elapsed_time\").filter(\n",
    "                    ((pl.col(\"event_name\") == 'navigate_click') & (pl.col(\"fqid\") == 'reader')) | (\n",
    "                            pl.col(\"fqid\") == \"reader.paper2.bingo\")).apply(lambda s: s.max() - s.min()).alias(\n",
    "                    \"reader_bingo_duration\"),\n",
    "                pl.col(\"index\").filter(((pl.col(\"event_name\") == 'navigate_click') & (pl.col(\"fqid\") == 'reader')) | (\n",
    "                        pl.col(\"fqid\") == \"reader.paper2.bingo\")).apply(lambda s: s.max() - s.min()).alias(\n",
    "                    \"reader_bingo_indexCount\"),\n",
    "                pl.col(\"elapsed_time\").filter(\n",
    "                    ((pl.col(\"event_name\") == 'navigate_click') & (pl.col(\"fqid\") == 'journals')) | (\n",
    "                            pl.col(\"fqid\") == \"journals.pic_2.bingo\")).apply(lambda s: s.max() - s.min()).alias(\n",
    "                    \"journals_bingo_duration\"),\n",
    "                pl.col(\"index\").filter(((pl.col(\"event_name\") == 'navigate_click') & (pl.col(\"fqid\") == 'journals')) | (\n",
    "                        pl.col(\"fqid\") == \"journals.pic_2.bingo\")).apply(lambda s: s.max() - s.min()).alias(\n",
    "                    \"journals_bingo_indexCount\"),\n",
    "            ]\n",
    "            #tmp = x.groupby(['session_id']).agg(aggs).sort_values(\"session_id\")\n",
    "\n",
    "            tmp = x.groupby([\"session_id\"], maintain_order=True).agg(aggs).sort(\"session_id\")\n",
    "            df = df.join(tmp, on=\"session_id\", how='left')\n",
    "\n",
    "        if grp == '13-22':\n",
    "            aggs = [\n",
    "                pl.col(\"elapsed_time\").filter(\n",
    "                    ((pl.col(\"event_name\") == 'navigate_click') & (pl.col(\"fqid\") == 'reader_flag')) | (\n",
    "                            pl.col(\"fqid\") == \"tunic.library.microfiche.reader_flag.paper2.bingo\")).apply(\n",
    "                    lambda s: s.max() - s.min() if s.len() > 0 else 0).alias(\"reader_flag_duration\"),\n",
    "                pl.col(\"index\").filter(\n",
    "                    ((pl.col(\"event_name\") == 'navigate_click') & (pl.col(\"fqid\") == 'reader_flag')) | (\n",
    "                            pl.col(\"fqid\") == \"tunic.library.microfiche.reader_flag.paper2.bingo\")).apply(\n",
    "                    lambda s: s.max() - s.min() if s.len() > 0 else 0).alias(\"reader_flag_indexCount\"),\n",
    "                pl.col(\"elapsed_time\").filter(\n",
    "                    ((pl.col(\"event_name\") == 'navigate_click') & (pl.col(\"fqid\") == 'journals_flag')) | (\n",
    "                            pl.col(\"fqid\") == \"journals_flag.pic_0.bingo\")).apply(\n",
    "                    lambda s: s.max() - s.min() if s.len() > 0 else 0).alias(\"journalsFlag_bingo_duration\"),\n",
    "                pl.col(\"index\").filter(\n",
    "                    ((pl.col(\"event_name\") == 'navigate_click') & (pl.col(\"fqid\") == 'journals_flag')) | (\n",
    "                            pl.col(\"fqid\") == \"journals_flag.pic_0.bingo\")).apply(\n",
    "                    lambda s: s.max() - s.min() if s.len() > 0 else 0).alias(\"journalsFlag_bingo_indexCount\")\n",
    "            ]\n",
    "            #tmp = x.groupby(['session_id']).agg(aggs).sort_values(\"session_id\")\n",
    "\n",
    "            tmp = x.groupby([\"session_id\"], maintain_order=True).agg(aggs).sort(\"session_id\")\n",
    "            df = df.join(tmp, on=\"session_id\", how='left')\n",
    "\n",
    "    return df.to_pandas(), x.with_columns(COLUMNS).to_pandas()\n",
    "\n",
    "def time_feature(train):\n",
    "    train = train.reset_index()\n",
    "    q = (\n",
    "        pl.from_pandas(train)\n",
    "          .lazy()\n",
    "          .with_columns([\n",
    "            pl.col(\"session_id\").apply(lambda x: int(str(x)[:2])).alias('year'),#.astype(np.uint8)\n",
    "            pl.col(\"session_id\").apply(lambda x: int(str(x)[2:4])+1).alias('month'),#.astype(np.uint8)\n",
    "            pl.col(\"session_id\").apply(lambda x: int(str(x)[4:6])).alias('day'),#.astype(np.uint8)\n",
    "            pl.col(\"session_id\").apply(lambda x: int(str(x)[6:8])).alias('hour'),#.astype(np.uint8)\n",
    "            pl.col(\"session_id\").apply(lambda x: int(str(x)[8:10])).alias('minute'),#.astype(np.uint8)\n",
    "            pl.col(\"session_id\").apply(lambda x: int(str(x)[10:12])).alias('second'),#.astype(np.uint8)\n",
    "            pl.col(\"session_id\").apply(lambda x: int(str(x)[12:])).alias('id_anonymous'),#.astype(np.uint8)\n",
    "        ])\n",
    "    )\n",
    "    # train[\"year\"] = train.with_columns(\"session_id\").apply(lambda x: int(str(x)[:2]))#.astype(np.uint8)\n",
    "    # train[\"month\"] = train[\"session_id\"].apply(lambda x: int(str(x)[2:4])+1)#.astype(np.uint8)\n",
    "    # train[\"day\"] = train[\"session_id\"].apply(lambda x: int(str(x)[4:6]))#.astype(np.uint8)\n",
    "    # train[\"hour\"] = train[\"session_id\"].apply(lambda x: int(str(x)[6:8]))#.astype(np.uint8)\n",
    "    # train[\"minute\"] = train[\"session_id\"].apply(lambda x: int(str(x)[8:10]))#.astype(np.uint8)\n",
    "    # train[\"second\"] = train[\"session_id\"].apply(lambda x: int(str(x)[10:12]))#.astype(np.uint8)\n",
    "    # train[\"id_anonymous\"] = train[\"session_id\"].apply(lambda x: int(str(x)[12:]))#.astype(np.uint8)\n",
    "    \n",
    "    # time features\n",
    "    # df = pl.from_pandas(train)\n",
    "    # aggs = [\n",
    "    #     pl.col('session_id').apply(lambda x: int(str(x)[:2])).alias('year'),\n",
    "    #     pl.col('session_id').apply(lambda x: int(str(x)[2:4])+1).alias('month'),\n",
    "    #     pl.col('session_id').apply(lambda x: int(str(x)[4:6])).alias('day'),\n",
    "    #     pl.col('session_id').apply(lambda x: int(str(x)[6:8])).alias('hour'),\n",
    "    #     pl.col('session_id').apply(lambda x: int(str(x)[8:10])).alias('minute'),\n",
    "    #     pl.col('session_id').apply(lambda x: int(str(x)[10:12])).alias('second'),\n",
    "    #     pl.col('session_id').apply(lambda x: int(str(x)[12:])).alias('id_anonymous'),\n",
    "    # ]\n",
    "\n",
    "    return q.collect().to_pandas().set_index('session_id')\n",
    "\n",
    "\n",
    "def new_page(X, grp):   # 여기서 더 일반적인 feature를 추출할 수 있을듯. page 0 을 들른 유저들은 new_page 0, 아닌 유저들은 1로 했는데, 그냥 조회한 page 수를 feature로 넘겨주면 되지않을까?\n",
    "    '''\n",
    "    X= revised_train dataset\n",
    "    '''\n",
    "    # 이상치 session_id 추출\n",
    "    if grp == '5-12':\n",
    "        session_2=X[X.page==0].session_id.unique().tolist()\n",
    "        X.loc[X['session_id'].isin(session_2), 'new_page'] = 0\n",
    "        X.loc[~X['session_id'].isin(session_2), 'new_page'] = 1\n",
    "    \n",
    "    if grp == '13-22':\n",
    "        session_3=X[(X.page==0)|(X.page==1)|(X.page==2)].session_id.unique().tolist()\n",
    "        X.loc[X['session_id'].isin(session_3), 'new_page'] = 0\n",
    "        X.loc[~X['session_id'].isin(session_3), 'new_page'] = 1\n",
    "    \n",
    "    return X.groupby(['session_id']).first().reset_index()\n",
    "\n",
    "# https://www.kaggle.com/code/glipko/recap-texts#Data-Extraction-\n",
    "def text_cnt(x, revised_train):\n",
    "    \n",
    "    x['in_the_same_dialogue'] = x['text_fqid'].shift() # 한칸씩 뒤로\n",
    "    x['in_the_same_dialogue'] = x['text_fqid'] == x['in_the_same_dialogue']\n",
    "\n",
    "    dialogue_sequence = x[~x['in_the_same_dialogue']] # 겹치지 않는 아이들\n",
    "    dialogue_sequence = dialogue_sequence[~dialogue_sequence['text_fqid'].isna()]\n",
    "    dialogue_sequence = dialogue_sequence[(dialogue_sequence['event_name'] == 'observation_click') | \\\n",
    "                                      (dialogue_sequence['event_name'] == 'person_click')]\n",
    "    dialogue_sequence = dialogue_sequence.drop(columns=['text', 'in_the_same_dialogue', 'elapsed_time'], errors='ignore')\n",
    "\n",
    "    dialogues = x[x['event_name'] == 'person_click']['text_fqid'].unique()\n",
    "    recap_dialogues = []\n",
    "    for dialogue in dialogues:\n",
    "        if('recap' in dialogue or 'lost' in dialogue):\n",
    "            recap_dialogues.append(dialogue)\n",
    "\n",
    "\n",
    "    observations = x[x['event_name'] == 'observation_click']['text_fqid'].unique()\n",
    "    recap_observations = []\n",
    "    for observation in observations:\n",
    "        if('block' in observation):\n",
    "            recap_observations.append(observation)\n",
    "\n",
    "\n",
    "    dialogue_sequence = dialogue_sequence[dialogue_sequence['text_fqid'].isin(recap_observations) | \\\n",
    "                        dialogue_sequence['text_fqid'].isin(recap_dialogues)]\n",
    "\n",
    "    session_event_recap = dialogue_sequence.groupby(['session_id', 'event_name']) \\\n",
    "                                        .size() \\\n",
    "                                        .reset_index() \\\n",
    "                                        .rename(columns={0:'recap_reading'})\n",
    "\n",
    "    session_event_recap = session_event_recap[(session_event_recap['event_name'] == 'observation_click') | \\\n",
    "                    (session_event_recap['event_name'] == 'person_click')]\n",
    "\n",
    "    session_recap = session_event_recap.groupby('session_id')['recap_reading'].sum()\n",
    "\n",
    "    texts = x[(~x['text'].isna()) & (x['text'] != 'undefined')]\n",
    "    reading = texts.groupby('session_id').size()\n",
    "    text_feature=pd.concat([session_recap,reading], axis=1)\n",
    "    text_feature.columns=['recap_reading', 'reading_cnt']\n",
    "    \n",
    "    # revised_train = pd.merge(x, revised_train, on='session_id', how='left')\n",
    "    \n",
    "    return pd.merge(revised_train, text_feature, on='session_id', how='left').set_index('session_id')\n",
    "\n",
    "\n",
    "def feature_quest(new_train, train, q):\n",
    "    train_q = new_train.copy()\n",
    "    texts = {\n",
    "        1: [\"Yes! This cool old slip from 1916.\", \n",
    "             \"Go ahead, take a peek at the shirt!\", \n",
    "             \"I'll be at the Capitol. Let me know if you find anything!\", \n",
    "             \"We need to talk about that missing paperwork.\", \n",
    "             \"The slip is from 1916 but the team didn't start until 1974!\"], \n",
    "         2: [\"It's already all done!\", \n",
    "             \"Gramps is the best historian ever!\"], \n",
    "         3: [\"I suppose historians are boring, too?\" \n",
    "             \"Why don't you head to the Basketball Center and rustle up some clues?\", \n",
    "             \"We need to talk about that missing paperwork.\"],    \n",
    "        \n",
    "         4: ['I need to find the owner of this slip.',\n",
    "             'She led marches and helped women get the right to vote!', \n",
    "             \"Here's a call number to find more info in the Stacks.\", \n",
    "             \"What was Wells doing here?\"],\n",
    "\n",
    "         5: [\"Your gramps is awesome! Always full of stories.\",\n",
    "             \"Here's a call number to find more info in the Stacks.\", \n",
    "             \"Where did you get that coffee?\"],         \n",
    "        \n",
    "         6: [\"Oh, that's from Bean Town.\", \n",
    "             \"Wells? I knew it!\"], \n",
    "           \n",
    "         7: [\"Try not to panic, Jo.\",\n",
    "             \"I've got a stack of business cards from my favorite cleaners.\",\n",
    "             \"Check out our microfiche. It's right through that door.\", \n",
    "             \"I'm afraid my papers have gone missing in this mess.\", \n",
    "             \"Nope. But Youmans and other suffragists worked hard to change that.\"], \n",
    "            \n",
    "         8: [\"What should I do first?\",\n",
    "             \"Thanks to them, Wisconsin was the first state to approve votes for women!\"], \n",
    "\n",
    "         9: [ \"Can you help me? I need to find the owner of this slip.\",\n",
    "             'Looks like a dry cleaning receipt.',\n",
    "             \"I knew I could count on you, Jo!\", \n",
    "             \"Nope, that's from Bean Town. I only drink Holdgers!\"], \n",
    "\n",
    "         10:[\"I love these photos of me and Teddy.\"\n",
    "             'Your gramps is awesome! Always full of stories.',\n",
    "             \"Nope. But Youmans and other suffragists worked hard to change that.\", \n",
    "             \"Right outside the door.\", \n",
    "             \"Do you have any info on Theodora Youmans?\"], \n",
    "                   \n",
    "         11:[\"I ran into Wells there this morning\",\n",
    "             'Your gramps is awesome! Always full of stories.',\n",
    "             \"Wait a sec. Women couldn't vote?!\", \n",
    "             \"I've got a stack of business cards from my favorite cleaners.\",\n",
    "             \"An old shirt? Try the university.\"],  \n",
    "         12:[],\n",
    "         13:[],        \n",
    "         14:[],\n",
    "         15:[],\n",
    "         16:[],\n",
    "         17:[],\n",
    "         18:[]\n",
    "        }\n",
    "    i = 0\n",
    "    for text in texts[q]:\n",
    "        i += 1\n",
    "        train_q['text' + str(i)] = train[train['text'] == text].groupby(['session_id'])['elapsed_time_diff'].sum()\n",
    "    \n",
    "    fqids = {\n",
    "         1: ['directory'], \n",
    "         2: ['notebook','chap1_finale_c'], \n",
    "         3: ['tostacks','doorblock'], \n",
    "         4: ['journals.pic_1.next', 'businesscards.card_1.next', 'block'], \n",
    "         5: ['janitor', 'journals.pic_2.next'], \n",
    "         6: ['businesscards', 'journals.pic_0.next','tobasement', 'logbook.page.bingo', 'tohallway'],  \n",
    "         7: ['journals.pic_1.next','reader.paper2.bingo','businesscards.card_bingo.next', \n",
    "             'logbook.page.bingo', 'tunic.kohlcenter'],  \n",
    "         8: ['reader.paper2.bingo'],  \n",
    "         9: ['journals.pic_1.next','businesscards.card_bingo.bingo', 'reader'],  \n",
    "         10:['tunic.kohlcenter','magnify','block','journals.pic_1.next', 'journals'], \n",
    "         11:['tostacks','block_magnify','block','businesscards.card_bingo.next'], \n",
    "         12:['businesscards.card_1.next','tofrontdesk'],  \n",
    "         13:['tocloset_dirty','reader.paper1.next'], \n",
    "         14:['tracks'], \n",
    "         15:['groupconvo_flag'], \n",
    "         16:['savedteddy'], \n",
    "         17:['journals_flag.pic_0.next'], \n",
    "         18:['chap4_finale_c'], \n",
    "        }\n",
    "    for fqid in fqids[q]:\n",
    "        train_q['t_fqid_' + fqid] = train[train['fqid'] == fqid].groupby(['session_id'])['elapsed_time_diff'].sum()\n",
    "\n",
    "    text_fqids = {\n",
    "        1:[],\n",
    "        2:['tunic.historicalsociety.collection.gramps.found'],\n",
    "        3:[],\n",
    "        4: ['tunic.humanecology.frontdesk.worker.intro',\n",
    "            'tunic.library.frontdesk.worker.wells', \n",
    "            'tunic.library.frontdesk.worker.hello'], \n",
    "        5: ['tunic.humanecology.frontdesk.worker.intro',\n",
    "            'tunic.historicalsociety.closet_dirty.gramps.helpclean',\n",
    "            'tunic.historicalsociety.closet_dirty.gramps.news'],     \n",
    "        6: ['tunic.humanecology.frontdesk.worker.intro',\n",
    "            'tunic.historicalsociety.frontdesk.archivist.foundtheodora',\n",
    "            'tunic.historicalsociety.closet_dirty.trigger_coffee', \n",
    "            'tunic.historicalsociety.closet_dirty.gramps.archivist'], \n",
    "        7: ['tunic.historicalsociety.closet_dirty.door_block_talk',\n",
    "            'tunic.drycleaner.frontdesk.worker.hub',\n",
    "            'tunic.historicalsociety.closet_dirty.trigger_coffee'], \n",
    "        8: ['tunic.humanecology.frontdesk.worker.intro',\n",
    "            'tunic.historicalsociety.frontdesk.magnify', \n",
    "            'tunic.historicalsociety.closet_dirty.trigger_coffee'], \n",
    "        9: ['tunic.historicalsociety.frontdesk.archivist.hello',\n",
    "            'tunic.library.frontdesk.worker.wells', \n",
    "            'tunic.historicalsociety.frontdesk.archivist.foundtheodora'], \n",
    "        10: ['tunic.library.frontdesk.worker.wells',\n",
    "            'tunic.historicalsociety.frontdesk.archivist.have_glass_recap',\n",
    "             'tunic.historicalsociety.closet_dirty.gramps.news'], \n",
    "        11: ['tunic.historicalsociety.frontdesk.archivist.newspaper_recap',\n",
    "             'tunic.historicalsociety.closet_dirty.gramps.archivist'], \n",
    "        12:[],\n",
    "        13:['tunic.drycleaner.frontdesk.logbook.page.bingo'],\n",
    "        14: ['tunic.flaghouse.entry.flag_girl.symbol_recap', \n",
    "             'tunic.historicalsociety.frontdesk.archivist_glasses.confrontation_recap'],\n",
    "        15:['tunic.flaghouse.entry.colorbook'], \n",
    "        16:['tunic.library.frontdesk.worker.nelson'], \n",
    "        17:['tunic.historicalsociety.entry.wells.flag'], \n",
    "        18:['tunic.flaghouse.entry.flag_girl.symbol_recap'], \n",
    "    }\n",
    "    for text_fqid in text_fqids[q]:\n",
    "        maska = train['text_fqid'] == text_fqid\n",
    "        train_q['t_text_fqid_' + text_fqid] = train[maska].groupby(['session_id'])['elapsed_time_diff'].sum()       \n",
    "        train_q['l_text_fqid_' + text_fqid] = train[train['text_fqid'] == text_fqid].groupby(['session_id'])['index'].count()\n",
    "\n",
    "\n",
    "    room_lvls = {\n",
    "         1: [['tunic.capitol_0.hall',4],['tunic.historicalsociety.collection',3],\n",
    "            ['tunic.historicalsociety.entry',1],['tunic.historicalsociety.collection', 2]], \n",
    "         2: [],\n",
    "         3: [['tunic.capitol_0.hall',4]], \n",
    "         4: [['tunic.historicalsociety.frontdesk',12], \n",
    "             ['tunic.historicalsociety.stacks',7]], \n",
    "         5: [['tunic.historicalsociety.stacks',12]],  \n",
    "         6: [['tunic.drycleaner.frontdesk',8],  \n",
    "             ['tunic.library.microfiche',9]], \n",
    "         7: [['tunic.library.frontdesk',10]], \n",
    "         8: [['tunic.kohlcenter.halloffame', 11], \n",
    "             ['tunic.kohlcenter.halloffame',6]], \n",
    "         9: [['tunic.capitol_1.hall', 12], \n",
    "             ['tunic.historicalsociety.collection',12]],\n",
    "         10:[['tunic.humanecology.frontdesk',7]], \n",
    "         11:[['tunic.drycleaner.frontdesk',9], \n",
    "             ['tunic.historicalsociety.collection',6]], \n",
    "         12:[['tunic.historicalsociety.stacks',6],\n",
    "             ['tunic.historicalsociety.frontdesk', 7],\n",
    "             ['tunic.historicalsociety.closet_dirty',11], \n",
    "             ['tunic.historicalsociety.frontdesk', 12]], \n",
    "         13:[['tunic.library.microfiche', 9], \n",
    "             ['tunic.historicalsociety.stacks', 11],\n",
    "             ['tunic.library.frontdesk', 10], \n",
    "             ['tunic.historicalsociety.entry', 5]], \n",
    "         14:[['tunic.historicalsociety.closet_dirty',17],\n",
    "             ['tunic.historicalsociety.entry',15]], \n",
    "         15:[['tunic.historicalsociety.entry',15],\n",
    "             ['tunic.library.frontdesk',20]], \n",
    "         16:[['tunic.library.frontdesk', 20],\n",
    "             ['tunic.wildlife.center',19]], \n",
    "         17:[['tunic.wildlife.center', 19],\n",
    "             ['tunic.historicalsociety.stacks', 21]], \n",
    "         18:[['tunic.wildlife.center', 22]], \n",
    "        }\n",
    "    for rl in room_lvls[q]:\n",
    "        nam = rl[0]+str(rl[1])\n",
    "        maska = (train['room_fqid'] == rl[0])&(train['level'] == rl[1])\n",
    "        train_q['t_' + nam] = train[maska].groupby(['session_id'])['elapsed_time_diff'].sum()\n",
    "        train_q['l_' + nam] = train[maska].groupby(['session_id'])['index'].count()\n",
    "\n",
    "    return train_q\n",
    "\n",
    "\n",
    "def load_targets(args):\n",
    "    targets = pd.read_parquet(args.target)\n",
    "    # targets = pd.read_csv(args.target)\n",
    "    targets[\"session\"] = targets[\"session_id\"].str.split(\"_\",expand = True)[0]\n",
    "    targets[\"session\"] = targets[\"session\"].astype(int)\n",
    "    targets['q'] = targets.session_id.apply(lambda x: int(x.split('_')[-1][1:]) )\n",
    "    return targets\n",
    "\n",
    "\n",
    "def preprocessing(df, grp):\n",
    "    # start, end = map(int,grp.split('-'))\n",
    "    # kol_lvl = (df.groupby(['session_id'])['level'].agg('nunique') < end - start + 1)\n",
    "    # list_session = kol_lvl[kol_lvl].index\n",
    "    # df = df[~df['session_id'].isin(list_session)]\n",
    "    # df = delt_time_def(df) # elapsed_time_diff feature와 겹치므로 이후 수정하기\n",
    "    train_, df = feature_engineer(pl.from_pandas(df), grp, use_extra=False, feature_suffix='')\n",
    "    # recap text count \\w join\n",
    "    train = text_cnt(df, train_)\n",
    "\n",
    "    # add year, month, day etc. - incorporate into feature_engineer\n",
    "    # train = time_feature(train)\n",
    "\n",
    "    \n",
    "    # df = new_page(df, grp)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return train, df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import platform\n",
    "import pickle\n",
    "\n",
    "def parse_args():\n",
    "\n",
    "    if platform.system() == 'Linux':\n",
    "        DIR = \"/home/wooseok/Python_lab/kaggle/gameplay/student-gameplay-prediction/\"\n",
    "    elif platform.system() == 'Darwin':\n",
    "        DIR = \"/Users/wooseokpark/github/kaggle/student-gameplay-prediction/\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # parser.add_argument(\"--target\", default=DIR + \"data/raw/input/train_labels.csv\", type=str, help=\"target csv\")\n",
    "    parser.add_argument(\"--target\", default=DIR + \"data/processed/concat/total_target_full_labeled.parquet\", type=str, help=\"target csv\")\n",
    "    # parser.add_argument(\"--train\", default=DIR + \"data/raw/input/train.parquet\", type=str, help=\"train parquet\")\n",
    "    parser.add_argument(\"--train\", default=DIR + \"data/processed/concat/total_train_fully_labeled.parquet\", type=str, help=\"train parquet\")\n",
    "    parser.add_argument(\"--model_path\", default=DIR + \"boosting/models/\", type=str, help=\"model path\")\n",
    "    parser.add_argument(\"--cv\", default=1, type=int, help=\"using cross-validation\")\n",
    "    # parser.add_argument(\"--nullcol\", default=DIR + \"boosting/processed/null_feat.npy\", type=str, help=\"null cols\")\n",
    "    parser.add_argument(\"--processed\", default=DIR + \"boosting/processed/\", type=str, help=\"processed path\")\n",
    "    parser.add_argument(\"--model\", default='xgb', type=str, help='which model')\n",
    "    parser.add_argument(\"--model_file\", default=DIR + \"boosting/processed/model.pkl\", type=str)\n",
    "    parser.add_argument(\"--result_file\", default=DIR + \"boosting/processed/result.pkl\", type=str)\n",
    "    parser.add_argument(\"--level_group\", default=\"0-4\", type=str)\n",
    "    parser.add_argument(\"--base_dir\", default=DIR, type=str, help=\"base directory\")\n",
    "    \n",
    "    args = parser.parse_args([])\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()\n",
    "\n",
    "targets = load_targets(args)\n",
    "df = pd.read_parquet(args.train)\n",
    "\n",
    "# remove outlier users\n",
    "# outliers = np.load(args.processed + 'outlier_users.npy')\n",
    "# df = df.set_index('session_id').drop(outliers).reset_index()\n",
    "\n",
    "args.level_group = '13-22'\n",
    "# save & load models and results\n",
    "# if args.level_group == '0-4':\n",
    "#     models = {}\n",
    "#     results = [[[], []] for _ in range(18)]\n",
    "# else:\n",
    "#     models = pickle.load(open(args.model_file, 'rb'))\n",
    "#     results = pickle.load(open(args.result_file, 'rb'))\n",
    "    \n",
    "\n",
    "list_q = {'0-4':[1,2,3], '5-12':[4,5,6,7,8,9,10,11,12,13], '13-22':[14,15,16,17,18]}\n",
    "# groups = ['0-4', '5-12', '13-22']\n",
    "\n",
    "# for grp in tqdm(groups):\n",
    "grp = args.level_group\n",
    "df_grp = df[df['level_group'] == grp]\n",
    "train, old_train = preprocessing(df_grp, grp)\n",
    "old_train = old_train[old_train['level_group'] == grp]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proper_features(train):\n",
    "    null = train.isnull().sum().sort_values(ascending=False) / len(train)\n",
    "    drop = list(null[null > 0.9].index)\n",
    "    for col in [c for c in train.columns if c not in drop]:\n",
    "        if train[col].nunique() == 1:\n",
    "            drop.append(col)\n",
    "    FEATURES = [c for c in train.columns if c not in drop]\n",
    "    return FEATURES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import GroupKFold, train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "        'booster': 'gbtree',\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric':'logloss',\n",
    "        'learning_rate': 0.02,\n",
    "        'alpha': 8,\n",
    "        'max_depth': 4,\n",
    "        'subsample':0.8,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'seed': 42\n",
    "        \n",
    "}\n",
    "\n",
    "models = {}\n",
    "# results = [[[], []] for _ in range(18)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 14\n",
      "Using columns: 3051\n",
      "Fold:1 2 3 4 5 "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tunic.historicalsociety.entry.groupconvo_flag_ET_quant25_</th>\n",
       "      <td>0.015267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunic.historicalsociety.entry.groupconvo_flag_ET_quant3_</th>\n",
       "      <td>0.014507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recap_reading</th>\n",
       "      <td>0.009020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunic.historicalsociety.entry.groupconvo_flag_ET_quant5_</th>\n",
       "      <td>0.007762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunic.historicalsociety.entry.groupconvo_flag_ET_median_</th>\n",
       "      <td>0.007734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tocloset_dirty_ET_max_min_</th>\n",
       "      <td>0.007446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tocloset_dirty_ET_std_</th>\n",
       "      <td>0.007146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunic.historicalsociety.frontdesk.archivist_glasses.confrontation_recap_ET_mean_</th>\n",
       "      <td>0.007084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_fqid_unique_</th>\n",
       "      <td>0.007053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunic.historicalsociety.frontdesk.archivist_glasses.confrontation_recap_ET_std_</th>\n",
       "      <td>0.007012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        mean\n",
       "feature                                                     \n",
       "tunic.historicalsociety.entry.groupconvo_flag_E...  0.015267\n",
       "tunic.historicalsociety.entry.groupconvo_flag_E...  0.014507\n",
       "recap_reading                                       0.009020\n",
       "tunic.historicalsociety.entry.groupconvo_flag_E...  0.007762\n",
       "tunic.historicalsociety.entry.groupconvo_flag_E...  0.007734\n",
       "tocloset_dirty_ET_max_min_                          0.007446\n",
       "tocloset_dirty_ET_std_                              0.007146\n",
       "tunic.historicalsociety.frontdesk.archivist_gla...  0.007084\n",
       "text_fqid_unique_                                   0.007053\n",
       "tunic.historicalsociety.frontdesk.archivist_gla...  0.007012"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 14 - Scores after 5 fold: F1: 0.54744 Precision: 0.73162 Recall: 0.96141\n",
      "Question: 15\n",
      "Using columns: 3049\n",
      "Fold:1 2 3 4 5 "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tunic.historicalsociety.entry.groupconvo_flag_ET_quant3_</th>\n",
       "      <td>0.021466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunic.historicalsociety.entry.groupconvo_flag_ET_quant25_</th>\n",
       "      <td>0.020997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recap_reading</th>\n",
       "      <td>0.011470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunic.historicalsociety.entry.groupconvo_flag_ET_quant5_</th>\n",
       "      <td>0.010970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunic.historicalsociety.entry.groupconvo_flag_ET_median_</th>\n",
       "      <td>0.009630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunic.wildlife.center.crane_ranger.crane_ET_std_</th>\n",
       "      <td>0.008562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunic.flaghouse.entry.flag_girl.symbol_recap_ET_max_min_</th>\n",
       "      <td>0.006768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crane_ranger_ET_quant8_</th>\n",
       "      <td>0.006731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunic.wildlife.center.crane_ranger.crane_ET_var_</th>\n",
       "      <td>0.006595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunic.wildlife.center.wells.nodeer_ET_quant25_</th>\n",
       "      <td>0.006131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        mean\n",
       "feature                                                     \n",
       "tunic.historicalsociety.entry.groupconvo_flag_E...  0.021466\n",
       "tunic.historicalsociety.entry.groupconvo_flag_E...  0.020997\n",
       "recap_reading                                       0.011470\n",
       "tunic.historicalsociety.entry.groupconvo_flag_E...  0.010970\n",
       "tunic.historicalsociety.entry.groupconvo_flag_E...  0.009630\n",
       "tunic.wildlife.center.crane_ranger.crane_ET_std_    0.008562\n",
       "tunic.flaghouse.entry.flag_girl.symbol_recap_ET...  0.006768\n",
       "crane_ranger_ET_quant8_                             0.006731\n",
       "tunic.wildlife.center.crane_ranger.crane_ET_var_    0.006595\n",
       "tunic.wildlife.center.wells.nodeer_ET_quant25_      0.006131"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 15 - Scores after 5 fold: F1: 0.64822 Precision: 0.62429 Recall: 0.65795\n",
      "Question: 16\n",
      "Using columns: 3049\n",
      "Fold:1 2 3 4 5 "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>map_hover_ET_sum_</th>\n",
       "      <td>0.003722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>map_hover_ET_kurtosis_</th>\n",
       "      <td>0.003357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>directory.closeup.archivist_ET_kurtosis_</th>\n",
       "      <td>0.003162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screen_coor_y_quant25_</th>\n",
       "      <td>0.003067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hover_duration_quant75_</th>\n",
       "      <td>0.002875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screen_coor_x_quant25_</th>\n",
       "      <td>0.002852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hover_duration_quant8_</th>\n",
       "      <td>0.002827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tocloset_dirty_ET_kurtosis_</th>\n",
       "      <td>0.002718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>directory.closeup.archivist_ET_skew_</th>\n",
       "      <td>0.002658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_quant75_the</th>\n",
       "      <td>0.002628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              mean\n",
       "feature                                           \n",
       "map_hover_ET_sum_                         0.003722\n",
       "map_hover_ET_kurtosis_                    0.003357\n",
       "directory.closeup.archivist_ET_kurtosis_  0.003162\n",
       "screen_coor_y_quant25_                    0.003067\n",
       "hover_duration_quant75_                   0.002875\n",
       "screen_coor_x_quant25_                    0.002852\n",
       "hover_duration_quant8_                    0.002827\n",
       "tocloset_dirty_ET_kurtosis_               0.002718\n",
       "directory.closeup.archivist_ET_skew_      0.002658\n",
       "word_quant75_the                          0.002628"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 16 - Scores after 5 fold: F1: 0.42036 Precision: 0.72242 Recall: 0.99956\n",
      "Question: 17\n",
      "Using columns: 3049\n",
      "Fold:1 2 3 4 5 "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word_mean_flag</th>\n",
       "      <td>0.005260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>close_ET_mean_</th>\n",
       "      <td>0.004023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person_click_ET_quant65_</th>\n",
       "      <td>0.003892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tracks_ET_max_min_</th>\n",
       "      <td>0.003356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>close_ET_sum_</th>\n",
       "      <td>0.003350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21_ET_std_</th>\n",
       "      <td>0.003142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunic.wildlife.center.crane_ranger.crane_ET_max_</th>\n",
       "      <td>0.003107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>notebook_click_ET_max_</th>\n",
       "      <td>0.003078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>archivist_glasses_ET_quant75_</th>\n",
       "      <td>0.003044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>notebook_click_ET_max_min_</th>\n",
       "      <td>0.002972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      mean\n",
       "feature                                                   \n",
       "word_mean_flag                                    0.005260\n",
       "close_ET_mean_                                    0.004023\n",
       "person_click_ET_quant65_                          0.003892\n",
       "tracks_ET_max_min_                                0.003356\n",
       "close_ET_sum_                                     0.003350\n",
       "21_ET_std_                                        0.003142\n",
       "tunic.wildlife.center.crane_ranger.crane_ET_max_  0.003107\n",
       "notebook_click_ET_max_                            0.003078\n",
       "archivist_glasses_ET_quant75_                     0.003044\n",
       "notebook_click_ET_max_min_                        0.002972"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 17 - Scores after 5 fold: F1: 0.40929 Precision: 0.68336 Recall: 0.99803\n",
      "Question: 18\n",
      "Using columns: 3045\n",
      "Fold:1 2 3 4 5 "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tunic.historicalsociety.entry.groupconvo_flag_ET_quant25_</th>\n",
       "      <td>0.008202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_quant25_Wells</th>\n",
       "      <td>0.007834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunic.historicalsociety.entry.groupconvo_flag_ET_quant3_</th>\n",
       "      <td>0.007743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunic.historicalsociety.entry.groupconvo_flag_ET_quant5_</th>\n",
       "      <td>0.007247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_quant3_Wells</th>\n",
       "      <td>0.007175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunic.historicalsociety.cage.confrontation_ET_quant25_</th>\n",
       "      <td>0.006487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunic.historicalsociety.entry.groupconvo_flag_ET_median_</th>\n",
       "      <td>0.006372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunic.library.frontdesk.worker.nelson_ET_quant25_</th>\n",
       "      <td>0.006152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>basic_ET_quant25_</th>\n",
       "      <td>0.006056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunic.wildlife.center.wells.animals_ET_quant25_</th>\n",
       "      <td>0.005706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        mean\n",
       "feature                                                     \n",
       "tunic.historicalsociety.entry.groupconvo_flag_E...  0.008202\n",
       "word_quant25_Wells                                  0.007834\n",
       "tunic.historicalsociety.entry.groupconvo_flag_E...  0.007743\n",
       "tunic.historicalsociety.entry.groupconvo_flag_E...  0.007247\n",
       "word_quant3_Wells                                   0.007175\n",
       "tunic.historicalsociety.cage.confrontation_ET_q...  0.006487\n",
       "tunic.historicalsociety.entry.groupconvo_flag_E...  0.006372\n",
       "tunic.library.frontdesk.worker.nelson_ET_quant25_   0.006152\n",
       "basic_ET_quant25_                                   0.006056\n",
       "tunic.wildlife.center.wells.animals_ET_quant25_     0.005706"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 18 - Scores after 5 fold: F1: 0.49003 Precision: 0.93669 Recall: 0.99962\n"
     ]
    }
   ],
   "source": [
    "feat_imp_dict = pickle.load(open(args.base_dir + \"data/features/feat_imp_dict_morefeat.pkl\", \"rb\"))\n",
    "pos_feat = pickle.load(open(args.base_dir + \"data/features/pos_feat_morefeat.pkl\", \"rb\"))\n",
    "\n",
    "# feat_imp_dict = {} # key: question number value: importance df \n",
    "# pos_feat = {} # positive importance features per question\n",
    "train_users = train.index.values\n",
    "st, en = list_q[args.level_group][0], list_q[args.level_group][-1]\n",
    "for q in range(st, en+1):\n",
    "    print(\"Question:\", q)\n",
    "    train_q = feature_quest(train, old_train, q)\n",
    "    FEATURES = proper_features(train_q)\n",
    "    train_q = train_q[FEATURES]\n",
    "    train_y = targets.loc[targets.q==q].set_index('session').loc[train_users]\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    f1_list, precision_list, recall_list = [], [], []\n",
    "    print('Using columns:', len(FEATURES))\n",
    "    print('Fold:', end= '')\n",
    "\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "\n",
    "    for k, (train_idx, val_idx) in enumerate(gkf.split(train_q, groups = train_users)):\n",
    "    # k = 0\n",
    "        print(k+1, end=' ')\n",
    "\n",
    "        X_train = train_q.iloc[train_idx]\n",
    "        X_val = train_q.iloc[val_idx]\n",
    "\n",
    "        # y_train = train_y['correct']\n",
    "        y_train = train_y.iloc[train_idx]['correct']\n",
    "        y_val = train_y.iloc[val_idx]['correct'].values\n",
    "\n",
    "        # if args.model == 'xgb':\n",
    "        model = XGBClassifier(\n",
    "            **xgb_params\n",
    "        )\n",
    "        model.fit(X_train, y_train, verbose=False)\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = train_q.columns\n",
    "        fold_importance_df[\"importance\"] = model.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = k + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        \n",
    "        \n",
    "        # SAVE MODEL\n",
    "        models[(k, q)] = model #fold, q\n",
    "\n",
    "        y_pred = model.predict_proba(X_val)[:,1]\n",
    "\n",
    "        # scores\n",
    "        f1 = f1_score(y_val, y_pred > 0.5, average='macro')\n",
    "        precision = precision_score(y_val, y_pred > 0.5)\n",
    "        recall = recall_score(y_val, y_pred > 0.5)\n",
    "        f1_list.append(f1); precision_list.append(precision); recall_list.append(recall)\n",
    "\n",
    "            # results[q - 1][0].append(y_val)\n",
    "            # results[q - 1][1].append(y_pred)\n",
    "\n",
    "    feature_importance_df = feature_importance_df.groupby(['feature'])['importance'].agg(['mean']).sort_values(by='mean', ascending=False)\n",
    "    display(feature_importance_df.head(10))\n",
    "    print()\n",
    "    print(f'Question {q} - Scores after {k+1} fold: F1: {np.mean(f1_list):.5f} Precision: {np.mean(precision_list):.5f} Recall: {np.mean(recall_list):.5f}')\n",
    "\n",
    "    # only using positive importance features\n",
    "    imp = feature_importance_df\n",
    "    FEAT_IMP = list(imp[imp['mean'] > 0].index)\n",
    "    pos_feat[q] = FEAT_IMP\n",
    "    # save imp df to dict\n",
    "    feat_imp_dict[q] = imp\n",
    "# save feat imp df and pos features\n",
    "pickle.dump(feat_imp_dict, open(args.base_dir + \"data/features/feat_imp_dict_morefeat.pkl\", \"wb\"))\n",
    "pickle.dump(pos_feat, open(args.base_dir + \"data/features/pos_feat_morefeat.pkl\", \"wb\"))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(feat_imp_dict, open(\"/home/wooseok/Python_lab/kaggle/gameplay/student-gameplay-prediction/data/features/feat_imp_dict.pkl\", \"wb\"))\n",
    "pickle.dump(pos_feat, open(\"/home/wooseok/Python_lab/kaggle/gameplay/student-gameplay-prediction/data/features/pos_feat.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1, 2, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_feat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 1714\n",
      "15 1570\n",
      "16 2140\n",
      "17 2137\n",
      "18 1792\n"
     ]
    }
   ],
   "source": [
    "for key in pos_feat.keys():\n",
    "    print(key, len(pos_feat[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 \n",
      "Question 15 - Scores after 5 fold: F1: 0.59728 Precision: 0.62338 Recall: 0.65895\n"
     ]
    }
   ],
   "source": [
    "# only positive importance features\n",
    "\n",
    "# feat_imp_dict = {} # key: question number value: importance df \n",
    "# train_users = train.index.values\n",
    "# train_y = targets.loc[targets.q==q].set_index('session').loc[train_users]\n",
    "# gkf = GroupKFold(n_splits=5)\n",
    "# f1_list, precision_list, recall_list = [], [], []\n",
    "# print('Fold:', end= '')\n",
    "\n",
    "# feature_importance_df = pd.DataFrame()\n",
    "train_q = train_q[FEAT_IMP]\n",
    "for k, (train_idx, val_idx) in enumerate(gkf.split(train_q, groups = train_users)):\n",
    "# k = 0\n",
    "    print(k+1, end=' ')\n",
    "\n",
    "    X_train = train_q.iloc[train_idx]\n",
    "    X_val = train_q.iloc[val_idx]\n",
    "\n",
    "    # y_train = train_y['correct']\n",
    "    y_train = train_y.iloc[train_idx]['correct']\n",
    "    y_val = train_y.iloc[val_idx]['correct'].values\n",
    "\n",
    "    # if args.model == 'xgb':\n",
    "    model = XGBClassifier(\n",
    "        **xgb_params\n",
    "    )\n",
    "    model.fit(X_train, y_train, verbose=False)\n",
    "    # fold_importance_df = pd.DataFrame()\n",
    "    # fold_importance_df[\"feature\"] = train_q.columns\n",
    "    # fold_importance_df[\"importance\"] = model.feature_importances_\n",
    "    # fold_importance_df[\"fold\"] = k + 1\n",
    "    # feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    \n",
    "    # SAVE MODEL\n",
    "    models[(k, q)] = model #fold, q\n",
    "\n",
    "    y_pred = model.predict_proba(X_val)[:,1]\n",
    "\n",
    "    # scores\n",
    "    f1 = f1_score(y_val, y_pred > 0.62, average='macro')\n",
    "    precision = precision_score(y_val, y_pred > 0.5)\n",
    "    recall = recall_score(y_val, y_pred > 0.5)\n",
    "    f1_list.append(f1); precision_list.append(precision); recall_list.append(recall)\n",
    "\n",
    "        # results[q - 1][0].append(y_val)\n",
    "        # results[q - 1][1].append(y_pred)\n",
    "\n",
    "# feature_importance_df = feature_importance_df.groupby(['feature'])['importance'].agg(['mean']).sort_values(by='mean', ascending=False)\n",
    "# # save imp df to dict\n",
    "# feat_imp_dict[q] = feature_importance_df\n",
    "print()\n",
    "print(f'Question {q} - Scores after {k+1} fold: F1: {np.mean(f1_list):.5f} Precision: {np.mean(precision_list):.5f} Recall: {np.mean(recall_list):.5f}')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1177\n"
     ]
    }
   ],
   "source": [
    "imp = feat_imp_dict[q]\n",
    "print((imp['mean'] > 0).sum())\n",
    "len(imp)\n",
    "\n",
    "FEAT_IMP = list(imp[imp['mean'] > 0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00050132, 0.        , 0.00064761, ..., 0.00059233, 0.00043256,\n",
       "       0.00091952], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "importance_df = pd.DataFrame()\n",
    "importance_df['feature'] = X_train.columns\n",
    "importance_df[\"importance\"] = model.feature_importances_\n",
    "\n",
    "cols = importance_df.loc[(importance_df.sort_values('importance', ascending=False)['importance'] != 0), 'feature'].values\n",
    "np.save('../data/features/g3_feature_posimp.npy', cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
